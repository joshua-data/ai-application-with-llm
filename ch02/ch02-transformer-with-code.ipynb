{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_list: ['나는', '최근', '파리', '여행을', '다녀왔다']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization with whitespaces\n",
    "input = '나는 최근 파리 여행을 다녀왔다'\n",
    "input_list = input.split()\n",
    "print('input_list:', input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2idx: {'나는': 0, '최근': 1, '파리': 2, '여행을': 3, '다녀왔다': 4}\n",
      "idx2word: {0: '나는', 1: '최근', 2: '파리', 3: '여행을', 4: '다녀왔다'}\n"
     ]
    }
   ],
   "source": [
    "# Tokens Dictionaries\n",
    "word2idx = {word:idx for idx, word in enumerate(input_list)}\n",
    "idx2word = {idx:word for idx, word in enumerate(input_list)}\n",
    "\n",
    "print('word2idx:', word2idx)\n",
    "print('idx2word:', idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Convert Tokens to Token IDs\n",
    "input_ids = [word2idx[word] for word in input_list]\n",
    "print('input_ids:', input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "embedding_layer = nn.Embedding(\n",
    "    len(word2idx),\n",
    "    embedding_dim\n",
    ") # (5, 16)\n",
    "\n",
    "input_embeddings = embedding_layer(torch.tensor(input_ids)) # (5, 16)\n",
    "input_embeddings = input_embeddings.unsqueeze(0) # (1, 5, 16)\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Absolute Positioning Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "max_position = 12\n",
    "\n",
    "embedding_layer = nn.Embedding(len(word2idx), embedding_dim) # (5, 16)\n",
    "positioning_embedding_layer = nn.Embedding(max_position, embedding_dim) # (12, 16)\n",
    "\n",
    "positioning_ids = torch.arange(len(input_ids), dtype=torch.long).unsqueeze(0) # (1, 5)\n",
    "positioning_encodings = positioning_embedding_layer(positioning_ids) # (1, 5, 16)\n",
    "\n",
    "token_embeddings = embedding_layer(torch.tensor(input_ids)) # (5, 16)\n",
    "token_embeddings = token_embeddings.unsqueeze(0) # (1, 5, 16)\n",
    "\n",
    "input_embeddings = token_embeddings + positioning_encodings\n",
    "print(input_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `nn.Linear` Layer creating query, key, and vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 16])\n",
      "torch.Size([1, 5, 16])\n",
      "torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "head_dim = 16\n",
    "\n",
    "weight_q = nn.Linear(embedding_dim, head_dim) # Input Size 16 - Linear Function - Output Size 16 \n",
    "weight_k = nn.Linear(embedding_dim, head_dim) # Input Size 16 - Linear Function - Output Size 16\n",
    "weight_v = nn.Linear(embedding_dim, head_dim) # Input Size 16 - Linear Function - Output Size 16\n",
    "\n",
    "queries = weight_q(input_embeddings) # (1, 5, 16)\n",
    "keys = weight_k(input_embeddings) # (1, 5, 16)\n",
    "values = weight_v(input_embeddings) # (1, 5, 16)\n",
    "\n",
    "print(queries.shape)\n",
    "print(keys.shape)\n",
    "print(values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention with Scaled Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_attention(queries, keys, values, is_causal=False):\n",
    "    embedding_dim = queries.size(-1) # 16\n",
    "    scores = queries @ keys.transpose(-2, -1) / sqrt(embedding_dim)\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return weights @ values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Embeddings & Output Embeddings while Computing Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Embeddings: torch.Size([1, 5, 16])\n",
      "Output Embeddings: torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "print('Input Embeddings:', input_embeddings.shape)\n",
    "\n",
    "output_embeddings = compute_attention(queries, keys, values)\n",
    "\n",
    "print('Output Embeddings:', output_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AttentionHead` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionHead(\n",
      "  (weight_q): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (weight_k): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (weight_v): Linear(in_features=16, out_features=16, bias=True)\n",
      ")\n",
      "torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, head_dim, is_causal=False):\n",
    "        super().__init__()\n",
    "        self.is_causal = is_causal\n",
    "        self.weight_q = nn.Linear(embedding_dim, head_dim) # Input Size 16 - Linear Function - Output Size 16\n",
    "        self.weight_k = nn.Linear(embedding_dim, head_dim) # Input Size 16 - Linear Function - Output Size 16\n",
    "        self.weight_v = nn.Linear(embedding_dim, head_dim) # Input Size 16 - Linear Function - Output Size 16\n",
    "    \n",
    "    def forward(self, queries, keys, values):\n",
    "        output_embeddings = compute_attention(\n",
    "            self.weight_q(queries),\n",
    "            self.weight_k(keys),\n",
    "            self.weight_v(values),\n",
    "            is_causal=self.is_causal\n",
    "        )\n",
    "        return output_embeddings\n",
    "\n",
    "attention_head = AttentionHead(embedding_dim, embedding_dim)\n",
    "output_embeddings = attention_head(input_embeddings, input_embeddings, input_embeddings)\n",
    "print(attention_head)\n",
    "print(output_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, head_dim, n_head, is_causal=False):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.is_causal = is_causal\n",
    "        self.weight_q = nn.Linear(embedding_dim, head_dim) # Input Size 16 - Linear Function - Output Size 16\n",
    "        self.weight_k = nn.Linear(embedding_dim, head_dim) # Input Size 16 - Linear Function - Output Size 16\n",
    "        self.weight_v = nn.Linear(embedding_dim, head_dim) # Input Size 16 - Linear Function - Output Size 16\n",
    "        self.concat_linear = nn.Linear(head_dim, head_dim)\n",
    "    \n",
    "    def forward(self, queries, keys, values):\n",
    "        B, T, C = queries.size()\n",
    "        queries = self.weight_q(queries).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        keys = self.weight_q(keys).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        values = self.weight_q(values).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        attention = compute_attention(queries, keys, values, self.is_causal)\n",
    "        output_embeddings = attention.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        output_embeddings = self.concat_linear(output_embeddings)\n",
    "        return output_embeddings\n",
    "\n",
    "n_head = 4\n",
    "multihead_attention = MultiheadAttention(embedding_dim, embedding_dim, n_head)\n",
    "output_embeddings = multihead_attention(input_embeddings, input_embeddings, input_embeddings)\n",
    "print(output_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_x shape: torch.Size([1, 5, 16])\n",
      "Mean of All the 16 Dim Values: tensor([[ 0.3371,  0.5774,  0.5453, -0.6474, -0.1695, -0.5298,  0.5652, -0.2789,\n",
      "         -0.2505, -0.1851,  0.5560,  0.2587, -0.2790,  0.3025, -0.5617, -0.2402]])\n",
      "Std of All the 16 Dim Values: tensor([[0.7775, 0.8474, 0.6012, 0.4478, 1.1062, 1.2426, 0.7826, 0.6867, 0.7281,\n",
      "         1.0024, 0.9099, 1.3914, 1.5595, 1.0045, 0.7462, 1.5360]])\n"
     ]
    }
   ],
   "source": [
    "norm = nn.LayerNorm(embedding_dim) # 16\n",
    "norm_x = norm(input_embeddings) # (1, 5, 16)\n",
    "print('norm_x shape:', norm_x.shape)\n",
    "\n",
    "print('Mean of All the 16 Dim Values:', norm_x.mean(dim=1).data)\n",
    "print('Std of All the 16 Dim Values:', norm_x.std(dim=1).data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-forward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreLayerNormFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, head_dim, feedforward_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(head_dim, feedforward_dim)\n",
    "        self.linear2 = nn.Linear(feedforward_dim, head_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = nn.GELU()\n",
    "        self.norm = nn.LayerNorm(head_dim)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        x = self.norm(src)\n",
    "        x = x + self.linear2(self.dropout1(self.activation(self.linear1(x))))\n",
    "        x = self.dropout2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_dim, n_head, feedforward_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiheadAttention(head_dim, head_dim, n_head)\n",
    "        self.norm1 = nn.LayerNorm(head_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.feed_forward = PreLayerNormFeedForward(head_dim, feedforward_dim, dropout)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        norm_x = self.norm1(src)\n",
    "        attention_output = self.attention(norm_x, norm_x, norm_x)\n",
    "        x = src + self.dropout1(attention_output)\n",
    "        x = self.feed_forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList(\n",
    "        [copy.deepcopy(module) for i in range(N)]\n",
    "    )\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder_layer, layers_cnt):\n",
    "        super().__init__()\n",
    "        self.layers = get_clones(encoder_layer, layers_cnt)\n",
    "        self.layers_cnt = layers_cnt\n",
    "        self.norm = norm\n",
    "    \n",
    "    def forward(self, src):\n",
    "        output = src\n",
    "        for mod in self.layers:\n",
    "            output = mod(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask Attention in Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention(queries, keys, values, is_causal=False):\n",
    "\n",
    "    embedding_dim = queries.size(-1) # 16\n",
    "    scores = queries @ keys.transpose(-2, -1) / sqrt(embedding_dim) # (1, 5, 5)\n",
    "\n",
    "    if is_causal:\n",
    "        query_len = queries.size(2)\n",
    "        key_len = keys.size(2)\n",
    "        temp_mask = torch.ones(query_len, key_len, dtype=torch.bool).tril(diagonal=0)\n",
    "        scores = scores.masked_fill(temp_mask==False, float('-inf'))\n",
    "    \n",
    "    weights = F.softmax(scores, dim=-1) # (1, 5, 5)\n",
    "\n",
    "    return weights @ values # (1, 5, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-attention in Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, head_dim, n_head, feedforward_dim=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiheadAttention(head_dim, head_dim, n_head)\n",
    "        self.multihead_attention = MultiheadAttention(head_dim, head_dim, n_head)\n",
    "        self.feed_forward = PreLayerNormFeedForward(head_dim, feedforward_dim, dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(head_dim)\n",
    "        self.norm2 = nn.LayerNorm(head_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, target, encoder_output, is_causal=True):\n",
    "        \n",
    "        # Self Attention\n",
    "        x = self.norm1(target)\n",
    "        x = x + self.dropout1(self.self_attention(x, x, x, is_causal=is_causal))\n",
    "        # Cross-attention\n",
    "        x = self.norm2(x)\n",
    "        x = x + self.dropout2(self.multihead_attention(x, encoder_output, encoder_output))\n",
    "        # Feed-forward\n",
    "        x = self.feed_forward(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList(\n",
    "        [copy.deepcopy(module) for i in range(N)]\n",
    "    )\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, decoder_layer, layers_cnt):\n",
    "        super().__init__()\n",
    "        self.layers = get_clones(decoder_layer, layers_cnt)\n",
    "        self.layers_cnt = layers_cnt\n",
    "    \n",
    "    def forward(self, target, src):\n",
    "        output = target\n",
    "        for mod in self.layers:\n",
    "            output = mod(target, src)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
