{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers==4.40.1 datasets==2.19.0 huggingface_hub==0.23.0 -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Transformer COdes with BERT & GPT-2 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshuakim/.pyenv/versions/3.8.10/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/joshuakim/.pyenv/versions/3.8.10/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "input_str = 'What is Huggingface Transformers?'\n",
    "\n",
    "# BERT Model\n",
    "\n",
    "bert_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "input_enc = bert_tokenizer(input_str, return_tensors='pt')\n",
    "output_enc = bert_model(**input_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.3009,  0.0158,  0.0698,  ..., -0.3406,  0.5976,  0.5820],\n",
       "         [-0.1109,  0.0754, -0.1906,  ...,  0.2970,  0.4278, -0.0391],\n",
       "         [-0.5813, -0.0042,  0.4034,  ..., -0.2549,  0.2216,  0.8121],\n",
       "         ...,\n",
       "         [ 0.9971,  0.3301, -0.0688,  ..., -0.4873,  0.0168, -0.0345],\n",
       "         [-0.2394, -0.0573, -0.5885,  ..., -0.0415,  0.3123, -0.0288],\n",
       "         [ 0.7884,  0.4039,  0.0217,  ...,  0.3869, -0.4785, -0.4116]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-8.7247e-01, -3.1464e-01, -5.2733e-01,  7.3685e-01,  1.8460e-01,\n",
       "         -1.8295e-01,  8.9985e-01,  2.9762e-01, -3.9314e-01, -9.9994e-01,\n",
       "          7.1272e-02,  7.2618e-01,  9.7232e-01,  3.2968e-01,  9.1370e-01,\n",
       "         -7.5043e-01, -3.5754e-01, -5.6530e-01,  2.8968e-01, -6.4040e-01,\n",
       "          6.0199e-01,  9.9856e-01,  3.8675e-01,  2.9078e-01,  4.0431e-01,\n",
       "          7.7010e-01, -7.6515e-01,  9.1830e-01,  9.4834e-01,  7.2557e-01,\n",
       "         -7.3735e-01,  1.9867e-01, -9.8172e-01, -1.8334e-01, -4.9430e-01,\n",
       "         -9.8059e-01,  2.6051e-01, -7.4618e-01,  4.8118e-02,  5.9710e-02,\n",
       "         -8.1904e-01,  2.1572e-01,  9.9974e-01, -6.2062e-01,  1.3180e-02,\n",
       "         -3.7181e-01, -1.0000e+00,  2.5721e-01, -8.5921e-01,  3.5142e-01,\n",
       "          4.2221e-01,  2.7744e-02,  1.5076e-01,  3.6331e-01,  4.4038e-01,\n",
       "          2.4008e-02, -1.7561e-01,  6.5656e-02, -2.2944e-01, -5.1775e-01,\n",
       "         -6.1054e-01,  4.0539e-01, -4.9568e-01, -8.7416e-01,  5.6503e-01,\n",
       "          5.4685e-01, -9.5152e-02, -2.8588e-01, -6.3586e-02, -5.9148e-02,\n",
       "          8.8904e-01,  1.0488e-01,  2.8848e-01, -8.3510e-01,  1.3450e-01,\n",
       "          2.2096e-01, -6.5445e-01,  1.0000e+00, -5.5778e-01, -9.5749e-01,\n",
       "          4.4432e-01,  5.2114e-01,  5.8760e-01,  2.3121e-02,  3.6001e-01,\n",
       "         -1.0000e+00,  2.6260e-01, -6.2052e-02, -9.8500e-01,  2.5122e-01,\n",
       "          4.3635e-01, -1.6588e-01,  3.9133e-01,  5.3988e-01, -5.7578e-01,\n",
       "         -1.7328e-01, -8.3042e-02, -3.6534e-01, -1.3268e-01, -1.2891e-01,\n",
       "          2.1035e-02, -2.0756e-01, -2.4547e-01, -3.3774e-01,  2.4423e-01,\n",
       "         -4.7804e-01, -4.5389e-01,  3.9510e-01,  7.4779e-02,  6.1011e-01,\n",
       "          5.0728e-01, -3.0130e-01,  3.7723e-01, -9.3758e-01,  4.9906e-01,\n",
       "         -3.2958e-01, -9.8185e-01, -6.1150e-01, -9.7898e-01,  6.7381e-01,\n",
       "         -6.0385e-02, -2.6210e-01,  9.3328e-01,  1.9747e-01,  3.5069e-01,\n",
       "         -7.0666e-02, -3.4704e-01, -1.0000e+00, -2.7822e-01, -3.7445e-01,\n",
       "         -1.7645e-01, -2.1203e-01, -9.6432e-01, -9.4622e-01,  5.7054e-01,\n",
       "          9.4001e-01,  1.8941e-01,  9.9923e-01, -1.5256e-01,  9.1510e-01,\n",
       "          7.8499e-02, -5.6504e-01, -7.2291e-02, -4.5555e-01,  5.5808e-01,\n",
       "          1.8401e-01, -6.0199e-01,  2.3481e-01, -3.3689e-02, -2.7570e-01,\n",
       "         -5.3239e-01, -1.7641e-01, -4.0169e-01, -9.2079e-01, -3.1998e-01,\n",
       "          9.3809e-01, -7.1153e-02, -6.9783e-01,  6.7253e-02, -2.0274e-01,\n",
       "         -3.7808e-01,  8.4576e-01,  5.0007e-01,  3.6423e-01, -4.1740e-01,\n",
       "          3.6110e-01,  1.4876e-01,  3.5860e-01, -8.4838e-01,  2.6076e-01,\n",
       "          4.1683e-01, -3.1267e-01, -5.2371e-01, -9.6102e-01, -2.9391e-01,\n",
       "          5.4447e-01,  9.8303e-01,  7.4894e-01,  2.8395e-01,  1.4951e-01,\n",
       "         -2.6745e-01,  1.8009e-01, -9.3940e-01,  9.6656e-01, -1.0958e-01,\n",
       "          1.8235e-01,  1.6497e-01,  2.4865e-01, -8.2815e-01,  7.0496e-04,\n",
       "          8.1065e-01, -1.2265e-02, -8.1630e-01, -4.3830e-02, -4.7633e-01,\n",
       "         -3.8496e-01, -3.8118e-01,  4.5080e-01, -1.9782e-01, -3.4960e-01,\n",
       "          1.1252e-01,  8.9026e-01,  9.7578e-01,  6.9775e-01, -1.2651e-01,\n",
       "          5.7326e-01, -8.3355e-01, -4.0410e-01,  5.5685e-02,  1.7855e-01,\n",
       "          8.9322e-02,  9.8696e-01, -3.9309e-01, -7.7632e-02, -9.1122e-01,\n",
       "         -9.7244e-01,  5.5044e-03, -8.4574e-01, -2.4718e-02, -6.3462e-01,\n",
       "          4.9246e-01,  5.5000e-01,  1.1419e-01,  3.0862e-01, -9.6484e-01,\n",
       "         -7.7138e-01,  3.5477e-01, -2.1973e-01,  4.2418e-01, -2.4453e-01,\n",
       "          6.6965e-01,  6.5434e-01, -6.3527e-01,  8.0241e-01,  9.1417e-01,\n",
       "         -4.4787e-01, -6.7925e-01,  8.3256e-01, -2.6945e-01,  8.7207e-01,\n",
       "         -5.2282e-01,  9.8420e-01,  4.2696e-01,  4.8134e-01, -8.7702e-01,\n",
       "         -5.7352e-01, -8.7672e-01, -3.6707e-01,  7.0802e-02, -1.4201e-01,\n",
       "          6.3290e-01,  6.3346e-01,  2.9797e-01,  4.1055e-01, -6.0411e-01,\n",
       "          9.9386e-01, -7.6779e-01, -9.4760e-01,  2.8060e-01,  1.7029e-02,\n",
       "         -9.8417e-01,  3.4940e-01,  2.5224e-01, -4.0932e-01, -3.8472e-01,\n",
       "         -6.1061e-01, -9.4425e-01,  8.4996e-01,  1.8030e-01,  9.8473e-01,\n",
       "         -8.3625e-02, -8.9589e-01, -4.1394e-01, -8.9448e-01, -2.8790e-01,\n",
       "         -1.7622e-01,  1.2934e-01, -1.4377e-01, -9.4701e-01,  4.8915e-01,\n",
       "          5.5083e-01,  3.5019e-01, -5.7310e-01,  9.9580e-01,  9.9998e-01,\n",
       "          9.5247e-01,  8.9147e-01,  8.6635e-01, -9.9394e-01, -3.3779e-01,\n",
       "          9.9998e-01, -9.1078e-01, -1.0000e+00, -9.3294e-01, -5.2658e-01,\n",
       "          3.7828e-01, -1.0000e+00, -3.1776e-01,  7.3380e-02, -8.6886e-01,\n",
       "          3.6298e-01,  9.6532e-01,  9.8247e-01, -1.0000e+00,  8.0954e-01,\n",
       "          9.0122e-01, -6.6414e-01,  6.7350e-01, -3.1281e-01,  9.6308e-01,\n",
       "          5.0662e-01,  4.2693e-01, -2.2366e-01,  3.7596e-01, -8.1644e-01,\n",
       "         -8.3308e-01, -1.3696e-01, -4.6714e-01,  9.4581e-01,  1.8325e-01,\n",
       "         -7.8852e-01, -8.7354e-01, -5.8152e-02, -6.9013e-02, -3.4173e-01,\n",
       "         -9.4760e-01, -1.7820e-01, -1.3928e-01,  6.8355e-01,  4.5169e-02,\n",
       "          2.6814e-01, -6.8769e-01,  2.1210e-01, -2.3397e-01,  3.4735e-01,\n",
       "          6.8611e-01, -9.2963e-01, -5.2597e-01,  3.0077e-01, -3.7891e-01,\n",
       "         -4.7398e-01, -9.5464e-01,  9.4878e-01, -3.2371e-01,  3.0548e-01,\n",
       "          1.0000e+00, -1.6540e-01, -7.9525e-01,  5.4072e-01,  1.6751e-01,\n",
       "         -1.2475e-01,  1.0000e+00,  6.4986e-01, -9.5677e-01, -6.0413e-01,\n",
       "          3.7020e-01, -4.8581e-01, -4.7777e-01,  9.9866e-01, -2.5729e-01,\n",
       "         -2.4092e-01,  1.1121e-01,  9.6287e-01, -9.8189e-01,  9.3767e-01,\n",
       "         -8.7587e-01, -9.4305e-01,  9.3710e-01,  9.0091e-01, -3.8187e-01,\n",
       "         -6.1976e-01,  1.4977e-01, -5.8692e-01,  2.6118e-01, -9.5406e-01,\n",
       "          6.2279e-01,  4.8689e-01, -8.8540e-02,  8.5752e-01, -8.3492e-01,\n",
       "         -5.3720e-01,  2.5680e-01, -4.1266e-01,  2.2846e-01,  5.8122e-01,\n",
       "          4.3429e-01, -2.4578e-01,  1.2280e-01, -2.9919e-01, -3.1362e-01,\n",
       "         -9.6913e-01,  6.2338e-02,  1.0000e+00,  1.3627e-02, -1.2817e-01,\n",
       "         -3.8745e-01,  1.8794e-02, -1.8447e-01,  4.2568e-01,  5.1443e-01,\n",
       "         -2.2532e-01, -8.0441e-01,  1.4079e-01, -9.4743e-01, -9.8253e-01,\n",
       "          7.6432e-01,  1.4451e-01, -3.2291e-01,  9.9988e-01,  3.1823e-01,\n",
       "          1.5380e-01,  1.3672e-01,  7.6152e-01,  1.0233e-01,  5.8834e-01,\n",
       "          5.5959e-01,  9.6941e-01, -2.7468e-01,  6.1814e-01,  8.2141e-01,\n",
       "         -5.6555e-01, -2.7680e-01, -6.2172e-01, -8.9432e-02, -9.0074e-01,\n",
       "          1.2009e-01, -9.2751e-01,  9.4329e-01,  4.9252e-01,  2.7484e-01,\n",
       "          2.1843e-01,  6.6066e-02,  1.0000e+00, -2.2593e-01,  6.4518e-01,\n",
       "         -5.1134e-01,  8.2547e-01, -9.8957e-01, -7.8994e-01, -3.4983e-01,\n",
       "         -8.3619e-02, -3.6136e-01, -2.2800e-01,  2.0661e-01, -9.5846e-01,\n",
       "          3.5442e-01,  9.7109e-02, -9.7418e-01, -9.8264e-01,  2.9154e-01,\n",
       "          8.3737e-01,  6.6634e-02, -8.0501e-01, -6.1920e-01, -5.9091e-01,\n",
       "          1.7083e-01, -1.7838e-01, -9.0619e-01,  2.4375e-01, -1.7610e-01,\n",
       "          4.1765e-01, -2.2214e-01,  5.1794e-01,  5.8822e-01,  6.4995e-01,\n",
       "          1.3124e-01, -9.7190e-02, -8.3229e-02, -8.4713e-01,  8.4501e-01,\n",
       "         -7.9905e-01, -4.0447e-01, -2.0988e-01,  1.0000e+00, -5.1438e-01,\n",
       "          6.3233e-01,  7.9540e-01,  6.0073e-01, -1.3826e-01,  2.1753e-01,\n",
       "          6.5091e-01,  1.4618e-01, -6.2026e-01, -4.0101e-01, -7.0176e-01,\n",
       "         -3.3691e-01,  6.5601e-01, -8.1213e-02,  4.6828e-01,  6.5125e-01,\n",
       "          4.2083e-01,  1.4356e-01, -3.9187e-02, -4.4730e-02,  9.9797e-01,\n",
       "         -1.2616e-01,  5.6322e-02, -5.0923e-01,  7.9198e-03, -3.3530e-01,\n",
       "         -4.9022e-01,  1.0000e+00,  2.7262e-01, -1.3420e-01, -9.8275e-01,\n",
       "         -3.7159e-01, -8.9745e-01,  9.9994e-01,  8.0115e-01, -7.2103e-01,\n",
       "          6.0988e-01,  2.2475e-01, -1.7136e-01,  7.3667e-01, -1.3005e-01,\n",
       "         -2.5390e-01,  2.7347e-01,  9.6593e-02,  9.2964e-01, -4.7674e-01,\n",
       "         -9.5305e-01, -6.8485e-01,  3.5561e-01, -9.3918e-01,  9.9616e-01,\n",
       "         -4.4200e-01, -1.8582e-01, -4.1399e-01,  4.5030e-01,  6.3905e-01,\n",
       "         -1.7178e-01, -9.7449e-01, -5.1296e-02,  1.6738e-01,  9.3006e-01,\n",
       "          1.3585e-01, -5.9081e-01, -9.1515e-01,  1.1425e-01,  4.2950e-01,\n",
       "         -5.2346e-01, -8.7475e-01,  9.4331e-01, -9.7478e-01,  3.9603e-01,\n",
       "          1.0000e+00,  3.9404e-01, -6.0146e-01,  2.9374e-02, -5.1601e-01,\n",
       "          2.4861e-01,  2.4923e-01,  6.7270e-01, -9.2070e-01, -2.8287e-01,\n",
       "         -7.6750e-02,  1.7067e-01, -8.0916e-02,  1.2053e-01,  5.9641e-01,\n",
       "          1.7944e-01, -5.4069e-01, -5.6923e-01, -2.5089e-02,  4.6722e-01,\n",
       "          8.3220e-01, -3.5286e-01, -1.6219e-01,  9.4051e-02, -9.4971e-02,\n",
       "         -8.7677e-01, -2.7396e-01, -2.1794e-01, -9.9938e-01,  7.5556e-01,\n",
       "         -1.0000e+00, -1.7111e-01, -1.8512e-01, -2.3656e-01,  7.9198e-01,\n",
       "          2.8102e-01,  2.7825e-01, -6.7673e-01, -5.1072e-01,  6.6396e-01,\n",
       "          6.7959e-01, -1.5749e-01,  1.9009e-01, -6.7916e-01,  1.1148e-01,\n",
       "         -4.1113e-03, -7.4846e-03, -1.5137e-01,  8.2224e-01, -1.6503e-01,\n",
       "          1.0000e+00,  1.4966e-01, -6.3775e-01, -9.6420e-01,  2.2951e-01,\n",
       "         -2.5219e-01,  1.0000e+00, -9.1262e-01, -9.3021e-01,  2.5157e-01,\n",
       "         -6.5498e-01, -8.3972e-01,  2.0794e-01,  1.4597e-01, -5.7222e-01,\n",
       "         -5.5899e-01,  9.3297e-01,  9.1276e-01, -6.3061e-01,  3.3351e-01,\n",
       "         -3.3155e-01, -3.4310e-01,  5.5119e-02,  1.7480e-01,  9.7497e-01,\n",
       "          1.8068e-01,  8.6350e-01,  4.1295e-01,  6.8414e-02,  9.4591e-01,\n",
       "          2.3672e-01,  4.7458e-01,  5.9380e-02,  1.0000e+00,  2.6528e-01,\n",
       "         -8.8648e-01,  3.4260e-01, -9.7573e-01, -1.5098e-01, -9.4319e-01,\n",
       "          2.2633e-01,  2.1388e-01,  8.7123e-01, -9.6189e-02,  9.3925e-01,\n",
       "          2.4524e-02, -9.8375e-02, -1.0091e-01,  1.4681e-01,  3.8430e-01,\n",
       "         -8.7014e-01, -9.7812e-01, -9.7528e-01,  3.6960e-01, -3.8164e-01,\n",
       "         -4.7706e-02,  1.4460e-01,  1.0588e-01,  3.0843e-01,  4.1187e-01,\n",
       "         -1.0000e+00,  9.1188e-01,  3.6029e-01,  7.0125e-01,  9.3118e-01,\n",
       "          5.1771e-01,  2.7587e-01,  2.8873e-01, -9.7848e-01, -9.4348e-01,\n",
       "         -3.1992e-01, -2.0700e-01,  6.7060e-01,  6.4204e-01,  8.7050e-01,\n",
       "          3.0572e-01, -4.9178e-01, -2.6046e-01,  1.0768e-01, -5.5766e-01,\n",
       "         -9.8769e-01,  3.4500e-01, -1.2493e-01, -9.4611e-01,  9.4435e-01,\n",
       "         -2.2333e-01, -1.4135e-01,  1.9776e-01, -3.9046e-01,  9.3880e-01,\n",
       "          6.8928e-01,  3.8767e-01,  1.1025e-01,  5.1922e-01,  8.3273e-01,\n",
       "          9.4730e-01,  9.7341e-01, -5.7427e-01,  7.8560e-01, -6.8621e-02,\n",
       "          4.1192e-01,  6.9466e-01, -9.3774e-01,  2.3971e-01,  1.3474e-01,\n",
       "         -4.2771e-01,  1.6046e-01, -2.4939e-01, -9.7068e-01,  5.0688e-01,\n",
       "         -2.9860e-01,  5.1771e-01, -3.1591e-01,  1.0868e-01, -3.8843e-01,\n",
       "         -1.5632e-01, -6.4045e-01, -5.4585e-01,  6.6964e-01,  3.3962e-01,\n",
       "          8.5658e-01,  5.4788e-01, -1.0526e-01, -5.1888e-01, -1.6210e-01,\n",
       "         -5.4360e-01, -8.8314e-01,  8.7812e-01,  4.3410e-02, -2.1279e-01,\n",
       "          3.2289e-01, -1.8518e-01,  6.5461e-01, -6.4462e-02, -3.7191e-01,\n",
       "         -3.7232e-01, -7.5502e-01,  8.0540e-01, -1.7700e-01, -5.2501e-01,\n",
       "         -5.6918e-01,  3.7711e-01,  3.8391e-01,  9.9864e-01, -5.3484e-01,\n",
       "         -5.4315e-01, -7.2384e-02, -2.4831e-01,  4.2074e-01, -2.2462e-01,\n",
       "         -1.0000e+00,  2.4747e-01,  7.5571e-02,  3.7574e-01, -7.1563e-02,\n",
       "          3.3389e-01, -1.6045e-01, -9.6561e-01, -1.6314e-01,  7.3814e-02,\n",
       "          3.5186e-01, -4.4838e-01, -2.2300e-01,  5.7876e-01,  6.1218e-01,\n",
       "          7.0053e-01,  8.2005e-01,  1.1350e-01,  2.0501e-01,  6.4786e-01,\n",
       "         -2.2238e-01, -6.3388e-01,  8.9791e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Hugging Face Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model using `model_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model_id = 'klue/roberta-base'\n",
    "model = AutoModel.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model including Classifier Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_id = 'SamLowe/roberta-base-go_emotions'\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model with Classifier Head randomly reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_id = 'klue/roberta-base'\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = 'klue/roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [0, 9157, 7461, 2190, 2259, 8509, 2138, 1793, 2855, 5385, 2200, 20950, 2]\n",
      "token_type_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "text_str = '토크나이저는 텍스트를 토큰 단위로 나눈다'\n",
    "text_tok = tokenizer(text_str)\n",
    "for k, v in  text_tok.items():\n",
    "    print(k, ':', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 토크 ##나이 ##저 ##는 텍스트 ##를 토 ##큰 단위 ##로 나눈다 [SEP]\n",
      "[CLS] 토크나이저는 텍스트를 토큰 단위로 나눈다 [SEP]\n",
      "토크나이저는 텍스트를 토큰 단위로 나눈다\n"
     ]
    }
   ],
   "source": [
    "text_ids = text_tok['input_ids']\n",
    "\n",
    "print(' '.join(tokenizer.convert_ids_to_tokens(text_ids)))\n",
    "print(tokenizer.decode(text_ids))\n",
    "print(tokenizer.decode(text_ids, skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Multiple Sentences with Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [[0, 1656, 1141, 3135, 6265, 2], [0, 864, 1141, 3135, 6265, 2]]\n",
      "token_type_ids : [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]\n",
      "attention_mask : [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "input_list = ['첫 번째 문장', '두 번째 문장']\n",
    "input_tok = tokenizer(input_list)\n",
    "for k, v in input_tok.items():\n",
    "    print(k, ':', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Multiple Sentences as One Document with Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]]\n",
      "token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "input_list = [['첫 번째 문장', '두 번째 문장']]\n",
    "input_tok = tokenizer(input_list)\n",
    "for k, v in input_tok.items():\n",
    "    print(k, ':', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode Token IDs to Original String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS] 첫 번째 문장 [SEP]', '[CLS] 두 번째 문장 [SEP]']\n",
      "['[CLS] 첫 번째 문장 [SEP] 두 번째 문장 [SEP]']\n"
     ]
    }
   ],
   "source": [
    "input_list = ['첫 번째 문장', '두 번째 문장']\n",
    "input_tok = tokenizer(input_list)\n",
    "input_ids = input_tok['input_ids']\n",
    "\n",
    "print(tokenizer.batch_decode(input_ids))\n",
    "\n",
    "input_list = [['첫 번째 문장', '두 번째 문장']]\n",
    "input_tok = tokenizer(input_list)\n",
    "input_ids = input_tok['input_ids']\n",
    "\n",
    "print(tokenizer.batch_decode(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `token_type_ids`: BERT Tokenizer vs. RoBERTa Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT:\n",
      "input_ids : [[2, 1656, 1141, 3135, 6265, 3, 864, 1141, 3135, 6265, 3]]\n",
      "token_type_ids : [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]]\n",
      "attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "RoBERTa Base:\n",
      "input_ids : [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]]\n",
      "token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "RoBERTa:\n",
      "input_ids : [[0, 9502, 3645, 2, 2, 10815, 3645, 2]]\n",
      "attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "input_list = [['첫 번째 문장', '두 번째 문장']]\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "input_tok = bert_tokenizer(input_list)\n",
    "print('BERT:')\n",
    "for k, v in input_tok.items():\n",
    "    print(k, ':', v)\n",
    "\n",
    "input_list = [['첫 번째 문장', '두 번째 문장']]\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')\n",
    "input_tok = roberta_tokenizer(input_list)\n",
    "print('RoBERTa Base:')\n",
    "for k, v in input_tok.items():\n",
    "    print(k, ':', v)\n",
    "\n",
    "input_list = [['first sentence', 'second sentence']]\n",
    "en_roberta_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "input_tok = en_roberta_tokenizer(input_list)\n",
    "print('RoBERTa:')\n",
    "for k, v in input_tok.items():\n",
    "    print(k, ':', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `attention_mask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [[0, 1656, 1141, 3135, 6265, 2073, 1599, 2062, 18, 2, 1, 1, 1, 1, 1, 1], [0, 864, 1141, 3135, 6265, 2073, 1656, 1141, 3135, 6265, 3632, 831, 647, 2062, 18, 2]]\n",
      "token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "input_list = ['첫 번째 문장은 짧다.', '두 번째 문장은 첫 번째 문장 보다 더 길다.']\n",
    "input_tok = tokenizer(input_list, padding='longest')\n",
    "\n",
    "for k, v in input_tok.items():\n",
    "    print(k, ':', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset from KLUE MRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "klue_mrc_data = load_dataset('klue', 'mrc')\n",
    "# klue_mrc_train_data = load_dataset('klue', 'mrc', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset from Local Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Files\n",
    "from datasets import load_dataset\n",
    "data = load_dataset('csv', data_files='my_files.csv')\n",
    "\n",
    "# Pandas Dictionaries\n",
    "from datasets import Dataset\n",
    "my_dict = {'a': [1, 2, 3]}\n",
    "data = Dataset.from_dict(my_dict)\n",
    "\n",
    "# Pandas Dataframe\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "data_df = pd.DataFrame(my_dict)\n",
    "data = Dataset.from_pandas(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download YNAT Datasets to Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_data = load_dataset('klue', 'ynat', split='train')\n",
    "eval_data = load_dataset('klue', 'ynat', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': 'ynat-v1_train_00000',\n",
       " 'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영',\n",
       " 'label': 3,\n",
       " 'url': 'https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=105&sid2=227&oid=001&aid=0008508947',\n",
       " 'date': '2016.06.30. 오전 10:36'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IT과학', '경제', '사회', '생활문화', '세계', '스포츠', '정치']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.features['label'].names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'label'],\n",
       "    num_rows: 45678\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.remove_columns(['guid', 'url', 'date'])\n",
    "eval_data = eval_data.remove_columns(['guid', 'url', 'date'])\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `label_str` Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영', 'label': 3, 'label_str': '생활문화'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_label_str(batch):\n",
    "    labels = train_data.features['label']\n",
    "    batch['label_str'] = labels.int2str(batch['label'])\n",
    "    return batch\n",
    "\n",
    "train_data = train_data.map(\n",
    "    make_label_str,\n",
    "    batched=True,\n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Train/Validate/Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.train_test_split(\n",
    "    test_size=10000,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")['test']\n",
    "\n",
    "eval_data = eval_data.train_test_split(\n",
    "    test_size=1000,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_data = eval_data['train'].train_test_split(\n",
    "    test_size=1000,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")['test']\n",
    "test_data = eval_data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: 10000\n",
      "Validate Dataset: 1000\n",
      "Test Dataset: 1000\n"
     ]
    }
   ],
   "source": [
    "print('Train Dataset:', len(train_data))\n",
    "print('Validate Dataset:', len(val_data))\n",
    "print('Test Dataset:', len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model with Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshuakim/.pyenv/versions/3.8.10/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d7a987c6424387b82f3d5ef9c23d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer\n",
    ")\n",
    "import torch\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(\n",
    "        examples['title'],\n",
    "        padding='max_length',\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "model_id = 'klue/roberta-base'\n",
    "targets_cnt = len(train_data.features['label'].names)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=targets_cnt\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "train_data = train_data.map(tokenize, batched=True)\n",
    "val_data = val_data.map(tokenize, batched=True)\n",
    "test_data = test_data.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=5e-5,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {'accuracy': (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d3e03c31dd4972bc2bf5a9a8934d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6584, 'grad_norm': 44.51023864746094, 'learning_rate': 3e-05, 'epoch': 0.4}\n",
      "{'loss': 0.5223, 'grad_norm': 29.636672973632812, 'learning_rate': 1e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75405ae5c7584f9183abfb9ddb5d6e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.524187445640564, 'eval_accuracy': 0.837, 'eval_runtime': 114.8583, 'eval_samples_per_second': 8.706, 'eval_steps_per_second': 1.088, 'epoch': 1.0}\n",
      "{'train_runtime': 4235.7883, 'train_samples_per_second': 2.361, 'train_steps_per_second': 0.295, 'train_loss': 0.5675425598144531, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02686de1f6cb4ef59adafca2fb3eeab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4738537073135376,\n",
       " 'eval_accuracy': 0.852,\n",
       " 'eval_runtime': 129.5776,\n",
       " 'eval_samples_per_second': 7.717,\n",
       " 'eval_steps_per_second': 0.965,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=hyperparams,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "trainer.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model without Trainer API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (1) Prepare Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshuakim/.pyenv/versions/3.8.10/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(\n",
    "        examples['title'],\n",
    "        padding='max_length',\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_id = 'klue/roberta-base'\n",
    "targets_cnt = len(train_data.features['label'].names)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=targets_cnt\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prepare Data to Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ce3bc9c62a45029b3090df3e9188be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_dataloader(dataset, batch_size, shuffle=True):\n",
    "    dataset = dataset.map(tokenize, batched=True).with_format('torch')\n",
    "    dataset = dataset.rename_column('label', 'labels')\n",
    "    dataset = dataset.remove_columns(column_names=['title'])\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "train_dataloader = make_dataloader(train_data, batch_size=8, shuffle=True)\n",
    "val_dataloader = make_dataloader(val_data, batch_size=8, shuffle=True)\n",
    "test_dataloader = make_dataloader(test_data, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define Function to Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)                \n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define Function to Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    preds = []\n",
    "    actual_labels = []    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )                                    \n",
    "            logits = outputs.logits\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            pred = torch.argmax(logits, dim=-1)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            actual_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = np.mean(\n",
    "        np.array(preds) == np.array(actual_labels)\n",
    "    )\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Start to Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshuakim/.pyenv/versions/3.8.10/lib/python3.8/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0889fe59d944272bf5c353a9fab7a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate Loss: {1.9003448028564454}\n",
      "Validate Accuracy: {0.386}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8974a3d216bf4378b8e67b5f8bb6d459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.398\n"
     ]
    }
   ],
   "source": [
    "epochs_cnt = 1\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(epochs_cnt):\n",
    "    print(f'Epoch {epoch + 1} / {epochs_cnt}')    \n",
    "    valid_loss, valid_accuracy = evaluate(model, val_dataloader)\n",
    "    print(f'Validate Loss:', {valid_loss})\n",
    "    print(f'Validate Accuracy:', {valid_accuracy})\n",
    "\n",
    "_, test_accuracy = evaluate(model, test_dataloader)\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Model to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {id:label for id, label in enumerate(train_data.features['label'].names)}\n",
    "label2id = {label:id for id, label in id2label.items()}\n",
    "\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/joshuakim/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57add6ddd7542e1a8a87c71d38d4fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8fc71dacb845e0abd6f71a042c74ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78050a8e814d475b851f9a982cecd451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b77fe27b02745d6902c3d1d6e0521f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a59fafc6954c53bc6a39b0a4b4e1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/joshua-data/roberta-base-klue-ynat-classification/commit/98ae2bc2658cdc44f088aedbb525135bb4d406cd', commit_message='Upload tokenizer', commit_description='', oid='98ae2bc2658cdc44f088aedbb525135bb4d406cd', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()\n",
    "huggingface_token = os.getenv('HUGGINGFACE_TOKEN')\n",
    "repo_id = 'joshua-data/roberta-base-klue-ynat-classification'\n",
    "\n",
    "login(token=huggingface_token)\n",
    "\n",
    "# Trained by Trainer API\n",
    "trainer.push_to_hub(repo_id)\n",
    "\n",
    "# Trained by Self Trained Model\n",
    "model.push_to_hub(repo_id)\n",
    "tokenizer.push_to_hub(repo_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshuakim/.pyenv/versions/3.8.10/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3fa5bcacce4762badecbd9caa8b372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03a3e1eac9f4adfb0fe843d8c73f838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa973e7bed6e4800b3f32af8cd7ee8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.28k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9aa08a16cde42268ae9f0f9d96be1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8ec5e189264f12be692c55ac69876a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/752k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd14831d45134555a89c1883e6343cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/971 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': '사회', 'score': 0.16757720708847046},\n",
       " {'label': '사회', 'score': 0.17347490787506104},\n",
       " {'label': '사회', 'score': 0.1690426915884018},\n",
       " {'label': '사회', 'score': 0.1701769381761551},\n",
       " {'label': '사회', 'score': 0.17067451775074005}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    'klue',\n",
    "    'ynat',\n",
    "    split='validation'\n",
    ")\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = 'joshua-data/roberta-base-klue-ynat-classification'\n",
    "model_pipeline = pipeline(\n",
    "    'text-classification',\n",
    "    model=model_id\n",
    ")\n",
    "model_pipeline(\n",
    "    dataset['title'][:5]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With Custom Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '사회', 'score': 0.16757717728614807},\n",
       " {'label': '사회', 'score': 0.17347487807273865},\n",
       " {'label': '사회', 'score': 0.1690426915884018},\n",
       " {'label': '사회', 'score': 0.1701769083738327},\n",
       " {'label': '사회', 'score': 0.17067457735538483}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "class CustomPipeline:\n",
    "    \n",
    "    def __init__(self, model_id):\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)        \n",
    "        self.model.eval()\n",
    "\n",
    "    def __call__(self, texts):\n",
    "        tokenized = self.tokenizer(\n",
    "            texts,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**tokenized)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        probs = softmax(logits, dim=-1)\n",
    "        scores, labels = torch.max(probs, dim=-1)\n",
    "        labels_str = [self.model.config.id2label[label_idx] for label_idx in labels.tolist()]                \n",
    "\n",
    "        return [\n",
    "            {'label': label, 'score': score.item()}\n",
    "            for label, score in zip(labels_str, scores)\n",
    "        ]\n",
    "\n",
    "custom_pipeline = CustomPipeline(model_id)\n",
    "custom_pipeline(dataset['title'][:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
